window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "venumML", "modulename": "venumML", "kind": "module", "doc": "<div style=\"text-align: center;\">\n    <img src=\"https://raw.githubusercontent.com/Vaultree/venumML/main/Vaultree_EML.png\" alt=\"Logo\" style=\"width: 100%;\">\n</div>\n\n<p><code>VENumML</code> is a Privacy Preserving Machine Learning (PPML) library designed for building and applying machine learning models on encrypted data. With Vaultree's <code>VENumpy</code> library, <code>VENumML</code> leverages fully homomorphic encryption (FHE) techniques to perform computations on encrypted data without decryption, ensuring data privacy throughout the machine learning workflow. This repo is available to install via <a href=\"https://pypi.org/project/venumML/\">PyPI</a>, see the installation instructions below for further details.</p>\n\n<p>Explore the <code>VENumML</code> <a href=\"https://docs.eml.vaultree.com\">Documentation</a> to learn more about our tool. Visit our <a href=\"https://www.github.com/Vaultree/venumML\">GitHub Repository</a> to access the codebase or check out the <a href=\"https://github.com/Vaultree/venumML/tree/main/demos\">demos</a> showcasing the capabilities of <code>VENumML</code>.</p>\n\n<h2 id=\"venumml-key-features\"><strong><code>VENumML</code> Key Features</strong></h2>\n\n<ul>\n<li><strong>Encrypted Machine Learning:</strong> Implement various machine learning models while keeping the underlying data encrypted.</li>\n<li><strong>Homomorphic Encryption Support:</strong> Works with Vaultree's <code>VENumpy</code> library that provides FHE functionalities.</li>\n<li><strong>Privacy-Preserving Predictions:</strong> Make predictions on encrypted data without revealing the original features.</li>\n</ul>\n\n<h2 id=\"modules\"><strong>Modules</strong></h2>\n\n<p>The <code>VENumML</code> library is under active development and currently includes implementations for:</p>\n\n<ul>\n<li>Linear Models\n<ul>\n<li><a href=\"https://docs.eml.vaultree.com/venumML/linear_models/regression/linear_regression.html\">Linear Regression</a>: Train and use a linear regression model on encrypted data for continuous target variables.</li>\n<li><a href=\"https://docs.eml.vaultree.com/venumML/linear_models/regression/logistic_regression.html\">Logistic Regression</a>: A logistic regression model on encrypted data for binary classification tasks.</li>\n</ul></li>\n<li>Optimization\n<ul>\n<li><a href=\"https://docs.eml.vaultree.com/venumML/optimization/sgd.html\">Stochastic gradient descent (SGD)</a>: An implementation of Nesterov's Accelerated Gradient Descent on encrypted data.</li>\n</ul></li>\n<li>Time Series (Phineus)\n<ul>\n<li><a href=\"https://docs.eml.vaultree.com/venumML/time_series/Phineus/phineus_FFT.html\">Fast Fourier Transform</a>: Perform FFT on encrypted time series data to analyze frequency domain information while preserving privacy.</li>\n<li><a href=\"https://docs.eml.vaultree.com/venumML/time_series/Phineus/phineus_timeseries.html\">Time Series</a>: Calculate rolling averages on encrypted time series data for smoothing and trend analysis.</li>\n</ul></li>\n<li>Deep Learning\n<ul>\n<li><a href=\"https://docs.eml.vaultree.com/venumML/deep_learning/transformer/transformer.html\">Transformers</a>: Explore encrypted implementations of the Transformer architecture for various deep learning tasks.</li>\n</ul></li>\n<li>Graphs\n<ul>\n<li><a href=\"https://docs.eml.vaultree.com/venumML/graphs/venum_graph.html\">Venum Graphs</a>: Create, or import from a <code>pandas</code> dataframe or <code>NetworkX</code> object to an encrypted node-edge graph data structure and a built-in PageRank score function.</li>\n</ul></li>\n<li>Approximation Functions\n<ul>\n<li><a href=\"https://docs.eml.vaultree.com/venumML/approx_functions.html\">Approximation Functions</a>: Approximation functions for softmax, tanh and sigmoid activation functions.</li>\n</ul></li>\n</ul>\n\n<h2 id=\"installation\"><strong>Installation</strong></h2>\n\n<p>The current version of <code>VENumML</code> supports Python 3.10/3.11 running on MacOS or Linux. It is recommeneded to install <code>VENumML</code> in a virtual environment.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>python<span class=\"w\"> </span>-m<span class=\"w\"> </span>venv<span class=\"w\"> </span>&lt;env_name&gt;\n</code></pre>\n</div>\n\n<p>At installation, <code>pip install</code> will automatically select the correct version of <code>VENumML</code> for your platform:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"n\">venumML</span>\n</code></pre>\n</div>\n\n<p><strong>Manual Installation</strong></p>\n\n<p>For manual installation, we have the following wheels available for various systems on our <a href=\"https://github.com/Vaultree/venumML\">Github Repository</a>.</p>\n\n<p>If you prefer to install manually, pre-built wheel files are available for the following platforms:</p>\n\n<ul>\n<li>Linux: venumML-x.x.x-manylinux_2_31_x86_64.whl</li>\n<li>MacOS (Intel): venumML-x.x.x-macosx_x86_64.whl</li>\n<li>MacOS (ARM): venumML-x.x.x-macosx_arm64.whl</li>\n</ul>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>/path/to/venumML-x.x.x-&lt;platform&gt;.whl\n</code></pre>\n</div>\n\n<p>where <code>&lt;platform&gt;</code> should be the appropriate identifier (<code>manylinux</code>, <code>macosx_x86_64</code>, or <code>macosx_arm64</code>).</p>\n\n<h2 id=\"venumpy\"><strong><code>VENumpy</code></strong></h2>\n\n<p>Vaultree Encrypted Numbers, <code>VENumpy</code>, the Python FHE (Fully Homomorphic Encryption) library developed by Vaultree provides the underlying technology of <code>VENumML</code>.</p>\n\n<h2 id=\"demos\"><strong>Demos</strong></h2>\n\n<p>For sample usage of <code>VENumML</code>, please see the notebook demos on our <a href=\"https://github.com/Vaultree/venumML/tree/main/demos\">Github repository</a>.</p>\n\n<h2 id=\"support-and-contribution\"><strong>Support and Contribution</strong></h2>\n\n<p>Please read our <a href=\"https://docs.eml.vaultree.com/\">Docs</a>.</p>\n\n<p>For other support, bug reports, feature requests or contribution to the <code>VENumML</code> project, please contact us via our <a href=\"https://customer.support.vaultree.com/servicedesk/customer/portals\">Support Portal</a>.</p>\n\n<h2 id=\"faq\"><strong>FAQ</strong></h2>\n\n<ol>\n<li><p><strong>What scheme is this based on?</strong></p>\n\n<p>VENumML is backed by VENum (Vaultree Encryption Numbers) which is a Rust production-grade homomorphic encryption library.\nWe are open-sourcing a Python implementation of the latest version of the library that you can check out at <a href=\"https://github.com/Vaultree/venum-python-backend\">venum-python-backend</a>.\nThis is a simplified version which will continue to update so that it is at functional parity with our Rust library. Additionally the VenumML library will be updated as compatible features come online.</p></li>\n<li><p><strong>Can I use this project with Docker?</strong></p>\n\n<p>Yes, provided that the Docker environments match the requirements for <code>venumML</code>.</p></li>\n<li><p><strong>How do I build or package the project?</strong></p>\n\n<p><code>venumML</code> is built on top of our <code>VENumpy</code> library using our proprietary FHE technology, which is not publicly available at the moment.</p></li>\n<li><p><strong>Can I use this library in a production environment?</strong></p>\n\n<p>See <a href=\"#Note\">Note</a>.</p></li>\n</ol>\n\n<h2 id=\"note\"><strong>Note</strong></h2>\n\n<p>This is the community edition of our EML product. It is free to use under the BSD 3-Clause Clear license only for research, prototyping, and experimentation purposes, not for production purposes. For any commercial use of Vaultree\u2019s open source code, companies must purchase our commercial patent license. By installing this product, you agree that Vaultree will not be held liable for any adverse impact, whether due to commercial deployment or otherwise. We pledge to update this edition as we continue to develop our product. Stay tuned for future changes to our support for production use of this edition and we welcome your contributions to make change happen!</p>\n\n<h2 id=\"licence\"><strong>Licence</strong></h2>\n\n<p>This project is licensed under the BSD 3-Clause License. See the <a href=\"https://github.com/Vaultree/venumML/blob/main/LICENSE\">LICENSE</a> file for details.</p>\n"}, {"fullname": "venumML.approx_functions", "modulename": "venumML.approx_functions", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.approx_functions.softmax_approximation", "modulename": "venumML.approx_functions", "qualname": "softmax_approximation", "kind": "function", "doc": "<p>Approximates the softmax function for encrypted data using a Taylor series expansion.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to encrypt constants in the approximation.</li>\n<li><strong>x</strong> (encrypted array-like or encrypted float):\nThe encrypted input data.</li>\n<li><strong>D</strong> (encrypted float):\nEncrypted normalisation factor for the softmax function, usually the sum of exponentiated terms.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>encrypted array-like or encrypted float</strong>: The softmax approximation values for each input in x, encrypted.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">D</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.approx_functions.tanh_approximation", "modulename": "venumML.approx_functions", "qualname": "tanh_approximation", "kind": "function", "doc": "<p>Approximates the hyperbolic tangent (tanh) function for encrypted data \nusing Taylor series.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to encrypt constants in the approximation.</li>\n<li><strong>x</strong> (encrypted float):\nThe encrypted input value for the tanh function.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>encrypted float</strong>: The approximate tanh value of the input, encrypted.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.approx_functions.sigmoid_approximation", "modulename": "venumML.approx_functions", "qualname": "sigmoid_approximation", "kind": "function", "doc": "<p>Approximates the sigmoid function for encrypted data using a Taylor series expansion.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to encrypt constants in the approximation.</li>\n<li><strong>x</strong> (encrypted array-like or encrypted float):\nThe encrypted input data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>encrypted array-like or encrypted float</strong>: The sigmoid approximation values for each input in x, encrypted.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.deep_learning", "modulename": "venumML.deep_learning", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.deep_learning.transformer", "modulename": "venumML.deep_learning.transformer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.deep_learning.transformer.transformer", "modulename": "venumML.deep_learning.transformer.transformer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.deep_learning.transformer.transformer.Embeddings", "modulename": "venumML.deep_learning.transformer.transformer", "qualname": "Embeddings", "kind": "class", "doc": "<p>Handles embedding lookup for input tokens, returning embeddings with a specified dimension.</p>\n\n<h6 id=\"attributes\">Attributes</h6>\n\n<ul>\n<li><strong>embedding_matrix</strong> (np.ndarray):\nCustom embedding matrix, where each row corresponds to the embedding vector for a token.</li>\n<li><strong>d_model</strong> (int):\nDimensionality of each embedding vector.</li>\n</ul>\n"}, {"fullname": "venumML.deep_learning.transformer.transformer.Embeddings.__init__", "modulename": "venumML.deep_learning.transformer.transformer", "qualname": "Embeddings.__init__", "kind": "function", "doc": "<p>Initialises the Embeddings class with a custom embedding matrix.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>custom_embeddings</strong> (np.ndarray):\nPre-trained embedding matrix, shape (vocab_size, d_model).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">custom_embeddings</span></span>)</span>"}, {"fullname": "venumML.deep_learning.transformer.transformer.Embeddings.forward", "modulename": "venumML.deep_learning.transformer.transformer", "qualname": "Embeddings.forward", "kind": "function", "doc": "<p>Computes embeddings for a batch of input token sequences.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>x</strong> (np.ndarray):\nArray of token indices with shape [batch_size, seq_length].</li>\n<li><strong>batch_size</strong> (int):\nThe number of sequences in the batch.</li>\n<li><strong>max_seq_len</strong> (int):\nMaximum sequence length.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>np.ndarray</strong>: Array of embeddings with shape (batch_size, seq_length, d_model).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">batch_size</span>, </span><span class=\"param\"><span class=\"n\">max_seq_len</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.deep_learning.transformer.transformer.positional_encoding", "modulename": "venumML.deep_learning.transformer.transformer", "qualname": "positional_encoding", "kind": "function", "doc": "<p>Generates positional encoding for a sequence of given length and embedding dimension.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>max_seq_len</strong> (int):\nMaximum sequence length for which positional encoding is generated.</li>\n<li><strong>d_model</strong> (int):\nDimensionality of each embedding vector.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>np.ndarray</strong>: Array of shape (max_seq_len, d_model) with positional encodings for each position.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">max_seq_len</span>, </span><span class=\"param\"><span class=\"n\">d_model</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.deep_learning.transformer.transformer.scaled_dot_product_attention", "modulename": "venumML.deep_learning.transformer.transformer", "qualname": "scaled_dot_product_attention", "kind": "function", "doc": "<p>Computes scaled dot-product attention with encrypted attention weights.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>Q</strong> (np.ndarray):\nQuery matrix of shape (batch_size, num_heads, seq_length, d_k).</li>\n<li><strong>K</strong> (np.ndarray):\nKey matrix of shape (batch_size, num_heads, seq_length, d_k).</li>\n<li><strong>V</strong> (np.ndarray):\nValue matrix of shape (batch_size, num_heads, seq_length, d_v).</li>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to encrypt the attention scores.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>output</strong> (np.ndarray):\nOutput after applying attention weights to the values, shape (batch_size, num_heads, seq_length, d_v).</li>\n<li><strong>attention_weights</strong> (np.ndarray):\nEncrypted attention weights applied to each value.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">Q</span>, </span><span class=\"param\"><span class=\"n\">K</span>, </span><span class=\"param\"><span class=\"n\">V</span>, </span><span class=\"param\"><span class=\"n\">ctx</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.deep_learning.transformer.transformer.MultiHeadAttention", "modulename": "venumML.deep_learning.transformer.transformer", "qualname": "MultiHeadAttention", "kind": "class", "doc": "<p>Implements multi-head attention mechanism with separate attention heads and output projection.</p>\n\n<h6 id=\"attributes\">Attributes</h6>\n\n<ul>\n<li><strong>num_heads</strong> (int):\nNumber of attention heads.</li>\n<li><strong>W_Qs</strong> (list):\nList of query weight matrices, one per head.</li>\n<li><strong>b_Qs</strong> (list):\nList of query bias vectors, one per head.</li>\n<li><strong>W_Ks</strong> (list):\nList of key weight matrices, one per head.</li>\n<li><strong>b_Ks</strong> (list):\nList of key bias vectors, one per head.</li>\n<li><strong>W_Vs</strong> (list):\nList of value weight matrices, one per head.</li>\n<li><strong>b_Vs</strong> (list):\nList of value bias vectors, one per head.</li>\n<li><strong>W_O</strong> (np.ndarray):\nOutput weight matrix applied after concatenating head outputs.</li>\n<li><strong>b_O</strong> (np.ndarray):\nOutput bias vector applied after concatenating head outputs.</li>\n</ul>\n"}, {"fullname": "venumML.deep_learning.transformer.transformer.MultiHeadAttention.__init__", "modulename": "venumML.deep_learning.transformer.transformer", "qualname": "MultiHeadAttention.__init__", "kind": "function", "doc": "<p>Initialises the MultiHeadAttention class with the specified number of heads.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>num_heads</strong> (int):\nNumber of attention heads.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">num_heads</span></span>)</span>"}, {"fullname": "venumML.deep_learning.transformer.transformer.MultiHeadAttention.set_head_weights", "modulename": "venumML.deep_learning.transformer.transformer", "qualname": "MultiHeadAttention.set_head_weights", "kind": "function", "doc": "<p>Sets the weights and biases for a specific attention head.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>head_index</strong> (int):\nIndex of the attention head.</li>\n<li><strong>head_weights</strong> (dict):\nDictionary containing weights and biases for the specified head.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">head_index</span>, </span><span class=\"param\"><span class=\"n\">head_weights</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.deep_learning.transformer.transformer.MultiHeadAttention.set_output_weights", "modulename": "venumML.deep_learning.transformer.transformer", "qualname": "MultiHeadAttention.set_output_weights", "kind": "function", "doc": "<p>Sets the weights and biases for the output layer.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>W_O</strong> (np.ndarray):\nOutput weight matrix.</li>\n<li><strong>b_O</strong> (np.ndarray):\nOutput bias vector.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">W_O</span>, </span><span class=\"param\"><span class=\"n\">b_O</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.deep_learning.transformer.transformer.MultiHeadAttention.multi_head_attention", "modulename": "venumML.deep_learning.transformer.transformer", "qualname": "MultiHeadAttention.multi_head_attention", "kind": "function", "doc": "<p>Computes multi-head attention for the given query, key, and value matrices.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>Q</strong> (np.ndarray):\nQuery matrix of shape (batch_size, seq_length, d_model).</li>\n<li><strong>K</strong> (np.ndarray):\nKey matrix of shape (batch_size, seq_length, d_model).</li>\n<li><strong>V</strong> (np.ndarray):\nValue matrix of shape (batch_size, seq_length, d_model).</li>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to encrypt attention scores.</li>\n<li><strong>d_model</strong> (int):\nDimensionality of the model.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>np.ndarray</strong>: The output of multi-head attention, shape (batch_size, seq_length, d_model).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">Q</span>, </span><span class=\"param\"><span class=\"n\">K</span>, </span><span class=\"param\"><span class=\"n\">V</span>, </span><span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">d_model</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.deep_learning.transformer.transformer.PositionwiseFeedForwardNetwork", "modulename": "venumML.deep_learning.transformer.transformer", "qualname": "PositionwiseFeedForwardNetwork", "kind": "class", "doc": "<p>Implements a position-wise feed-forward network with two linear transformations \nand an activation function.</p>\n\n<h6 id=\"attributes\">Attributes</h6>\n\n<ul>\n<li><strong>d_model</strong> (int):\nDimensionality of the input.</li>\n<li><strong>d_ff</strong> (int):\nDimensionality of the hidden layer.</li>\n<li><strong>W_1</strong> (np.ndarray):\nWeight matrix for the first linear layer.</li>\n<li><strong>b_1</strong> (np.ndarray):\nBias vector for the first linear layer.</li>\n<li><strong>W_2</strong> (np.ndarray):\nWeight matrix for the second linear layer.</li>\n<li><strong>b_2</strong> (np.ndarray):\nBias vector for the second linear layer.</li>\n</ul>\n"}, {"fullname": "venumML.deep_learning.transformer.transformer.PositionwiseFeedForwardNetwork.__init__", "modulename": "venumML.deep_learning.transformer.transformer", "qualname": "PositionwiseFeedForwardNetwork.__init__", "kind": "function", "doc": "<p>Initialises the PositionwiseFeedForwardNetwork.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>d_model</strong> (int):\nDimensionality of the input.</li>\n<li><strong>d_ff</strong> (int):\nDimensionality of the hidden layer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">d_model</span>, </span><span class=\"param\"><span class=\"n\">d_ff</span></span>)</span>"}, {"fullname": "venumML.deep_learning.transformer.transformer.PositionwiseFeedForwardNetwork.set_weights", "modulename": "venumML.deep_learning.transformer.transformer", "qualname": "PositionwiseFeedForwardNetwork.set_weights", "kind": "function", "doc": "<p>Sets the weights and biases for the feed-forward network.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>W_1</strong> (np.ndarray):\nWeight matrix for the first linear layer.</li>\n<li><strong>b_1</strong> (np.ndarray):\nBias vector for the first linear layer.</li>\n<li><strong>W_2</strong> (np.ndarray):\nWeight matrix for the second linear layer.</li>\n<li><strong>b_2</strong> (np.ndarray):\nBias vector for the second linear layer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">W_1</span>, </span><span class=\"param\"><span class=\"n\">b_1</span>, </span><span class=\"param\"><span class=\"n\">W_2</span>, </span><span class=\"param\"><span class=\"n\">b_2</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.deep_learning.transformer.transformer.PositionwiseFeedForwardNetwork.forward", "modulename": "venumML.deep_learning.transformer.transformer", "qualname": "PositionwiseFeedForwardNetwork.forward", "kind": "function", "doc": "<p>Forward pass through the feed-forward network.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>x</strong> (np.ndarray):\nInput array with shape [batch_size, seq_length, d_model].</li>\n<li><strong>ctx</strong> (EncryptionContext):\nEncryption context used for any approximations in activation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>np.ndarray</strong>: Output of the feed-forward network.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">ctx</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.deep_learning.transformer.transformer.output_linear_layer", "modulename": "venumML.deep_learning.transformer.transformer", "qualname": "output_linear_layer", "kind": "function", "doc": "<p>Implements the output linear layer for a Transformer model.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>x</strong> (np.ndarray):\nInput array representing the output of the last Transformer layer.</li>\n<li><strong>W</strong> (np.ndarray):\nWeight matrix for the linear transformation.</li>\n<li><strong>b</strong> (np.ndarray):\nBias vector for the linear transformation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>np.ndarray</strong>: The output vector for each token in the vocabulary.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">W</span>, </span><span class=\"param\"><span class=\"n\">b</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.deep_learning.transformer.transformer.TransformerModule", "modulename": "venumML.deep_learning.transformer.transformer", "qualname": "TransformerModule", "kind": "class", "doc": "<p>A simplified Transformer module implementing embedding, multi-head attention, \npositional encoding, and a feed-forward network.</p>\n\n<h6 id=\"attributes\">Attributes</h6>\n\n<ul>\n<li><strong>max_seq_len</strong> (int):\nMaximum sequence length.</li>\n<li><strong>num_heads</strong> (int):\nNumber of attention heads.</li>\n<li><strong>d_model</strong> (int):\nDimensionality of embeddings.</li>\n<li><strong>MHA</strong> (MultiHeadAttention):\nMulti-head attention layer.</li>\n<li><strong>positional_encoding</strong> (np.ndarray):\nPositional encoding matrix.</li>\n<li><strong>p_ffn</strong> (PositionwiseFeedForwardNetwork):\nFeed-forward network.</li>\n<li><strong>output_w</strong> (np.ndarray):\nWeight matrix for the output layer.</li>\n<li><strong>output_b</strong> (np.ndarray):\nBias vector for the output layer.</li>\n</ul>\n"}, {"fullname": "venumML.deep_learning.transformer.transformer.TransformerModule.__init__", "modulename": "venumML.deep_learning.transformer.transformer", "qualname": "TransformerModule.__init__", "kind": "function", "doc": "<p>Initialises the TransformerModule with necessary layers and weights.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>encrypted_state_dict</strong> (dict):\nDictionary containing pre-trained weights in encrypted form.</li>\n<li><strong>max_seq_len</strong> (int):\nMaximum length of input sequences.</li>\n<li><strong>d_model</strong> (int):\nDimensionality of embeddings.</li>\n<li><strong>num_heads</strong> (int):\nNumber of attention heads.</li>\n<li><strong>d_ff</strong> (int):\nDimensionality of the feed-forward network's hidden layer.</li>\n<li><strong>vocab_size</strong> (int):\nSize of the vocabulary.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">encrypted_state_dict</span>,</span><span class=\"param\">\t<span class=\"n\">max_seq_len</span>,</span><span class=\"param\">\t<span class=\"n\">d_model</span>,</span><span class=\"param\">\t<span class=\"n\">num_heads</span>,</span><span class=\"param\">\t<span class=\"n\">d_ff</span>,</span><span class=\"param\">\t<span class=\"n\">vocab_size</span></span>)</span>"}, {"fullname": "venumML.deep_learning.transformer.transformer.TransformerModule.forward", "modulename": "venumML.deep_learning.transformer.transformer", "qualname": "TransformerModule.forward", "kind": "function", "doc": "<p>Processes the input embeddings through the Transformer.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>embeddings</strong> (np.ndarray):\nInput embeddings with shape [batch_size, seq_length, d_model].</li>\n<li><strong>ctx</strong> (EncryptionContext):\nEncryption context used for any approximations in activation.</li>\n<li><strong>batch_size</strong> (int):\nNumber of input sequences in the batch.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>np.ndarray</strong>: Output vector after embedding, positional encoding, attention, and the feed-forward network.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">embeddings</span>, </span><span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">batch_size</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.graphs", "modulename": "venumML.graphs", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.graphs.venum_graph", "modulename": "venumML.graphs.venum_graph", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.graphs.venum_graph.Graph", "modulename": "venumML.graphs.venum_graph", "qualname": "Graph", "kind": "class", "doc": "<p>Represents a Venum encrypted graph that supports encryption and decryption \nof the adjacency matrix for privacy-preserving computations.</p>\n\n<h6 id=\"attributes\">Attributes</h6>\n\n<ul>\n<li><strong>use_hashing</strong> (bool):\nIndicates whether to use hashing for node identifiers.</li>\n<li><strong>nodes</strong> (dict):\nMaps hashed node identifiers to integer indices.</li>\n<li><strong>directed</strong> (bool):\nSpecifies if the graph is directed.</li>\n<li><strong>adjacency_matrix</strong> (np.ndarray):\nEncrypted adjacency matrix representing edge weights.</li>\n<li><strong>boolean_matrix</strong> (np.ndarray):\nEncrypted adjacency matrix representing the presence of edges.</li>\n<li><strong>inverse_outbound</strong> (np.ndarray):\nEncrypted array of inverse outbound degree counts for each node.</li>\n<li><strong>boolean_outbound</strong> (np.ndarray):\nEncrypted boolean array indicating nodes with outbound edges.</li>\n</ul>\n"}, {"fullname": "venumML.graphs.venum_graph.Graph.__init__", "modulename": "venumML.graphs.venum_graph", "qualname": "Graph.__init__", "kind": "function", "doc": "<p>Initialises an encrypted graph with optional hashing for node identifiers.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to initialise matrices.</li>\n<li><strong>directed</strong> (bool, optional, default=True):\nWhether the graph is directed.</li>\n<li><strong>use_hashing</strong> (bool, optional, default=True):\nWhether to use SHA-256 hashing for node identifiers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">directed</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">use_hashing</span><span class=\"o\">=</span><span class=\"kc\">True</span></span>)</span>"}, {"fullname": "venumML.graphs.venum_graph.Graph.hash_node", "modulename": "venumML.graphs.venum_graph", "qualname": "Graph.hash_node", "kind": "function", "doc": "<p>Hashes a node using SHA-256.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>node</strong> (any):\nThe node identifier.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>str</strong>: The SHA-256 hexadecimal digest of the node.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">node</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.graphs.venum_graph.Graph.is_sha256_hash", "modulename": "venumML.graphs.venum_graph", "qualname": "Graph.is_sha256_hash", "kind": "function", "doc": "<p>Checks if the given value is a SHA-256 hash.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>value</strong> (str):\nThe value to check.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>bool</strong>: True if the value is a valid SHA-256 hash, False otherwise.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">value</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.graphs.venum_graph.Graph.add_node", "modulename": "venumML.graphs.venum_graph", "qualname": "Graph.add_node", "kind": "function", "doc": "<p>Adds a node to the graph after hashing and ensures the adjacency matrix is resized.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to encrypt values.</li>\n<li><strong>node</strong> (any):\nThe node identifier.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">node</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.graphs.venum_graph.Graph.add_edge", "modulename": "venumML.graphs.venum_graph", "qualname": "Graph.add_edge", "kind": "function", "doc": "<p>Adds a weighted edge between two nodes.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to encrypt values.</li>\n<li><strong>from_node</strong> (any):\nThe starting node of the edge.</li>\n<li><strong>to_node</strong> (any):\nThe ending node of the edge.</li>\n<li><strong>weight</strong> (float, optional, default=1):\nThe weight of the edge.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">from_node</span>, </span><span class=\"param\"><span class=\"n\">to_node</span>, </span><span class=\"param\"><span class=\"n\">weight</span><span class=\"o\">=</span><span class=\"mi\">1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.graphs.venum_graph.Graph.remove_edge", "modulename": "venumML.graphs.venum_graph", "qualname": "Graph.remove_edge", "kind": "function", "doc": "<p>Removes an edge between two nodes by setting the matrix value to 0.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>from_node</strong> (any):\nThe starting node of the edge.</li>\n<li><strong>to_node</strong> (any):\nThe ending node of the edge.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">from_node</span>, </span><span class=\"param\"><span class=\"n\">to_node</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.graphs.venum_graph.Graph.get_edge_weight", "modulename": "venumML.graphs.venum_graph", "qualname": "Graph.get_edge_weight", "kind": "function", "doc": "<p>Returns the weight of an edge between two nodes.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>from_node</strong> (any):\nThe starting node of the edge.</li>\n<li><strong>to_node</strong> (any):\nThe ending node of the edge.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>float or int</strong>: The weight of the edge, or 0 if no edge exists.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">from_node</span>, </span><span class=\"param\"><span class=\"n\">to_node</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.graphs.venum_graph.Graph.get_adjacency_matrix", "modulename": "venumML.graphs.venum_graph", "qualname": "Graph.get_adjacency_matrix", "kind": "function", "doc": "<p>Retrieves the encrypted adjacency matrix.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>np.ndarray</strong>: The current encrypted adjacency matrix.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.graphs.venum_graph.Graph.get_boolean_matrix", "modulename": "venumML.graphs.venum_graph", "qualname": "Graph.get_boolean_matrix", "kind": "function", "doc": "<p>Retrieves the encrypted boolean adjacency matrix.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>np.ndarray</strong>: The current encrypted boolean adjacency matrix.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.graphs.venum_graph.Graph.get_node_degree", "modulename": "venumML.graphs.venum_graph", "qualname": "Graph.get_node_degree", "kind": "function", "doc": "<p>Calculates the degree of a node.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>node</strong> (any):\nThe node identifier.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>int</strong>: The degree of the node.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">node</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.graphs.venum_graph.encrypt_networkx", "modulename": "venumML.graphs.venum_graph", "qualname": "encrypt_networkx", "kind": "function", "doc": "<p>Converts a NetworkX graph to an encrypted custom Graph.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to encrypt node and edge data.</li>\n<li><strong>nx_graph</strong> (networkx.Graph or networkx.DiGraph):\nThe NetworkX graph to convert.</li>\n<li><strong>use_hashing</strong> (bool, optional, default=True):\nWhether to hash node identifiers.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>Graph</strong>: An encrypted custom Graph with nodes, edges, and outbound counts encrypted.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">nx_graph</span>, </span><span class=\"param\"><span class=\"n\">use_hashing</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.graphs.venum_graph.df_to_encrypted_graph", "modulename": "venumML.graphs.venum_graph", "qualname": "df_to_encrypted_graph", "kind": "function", "doc": "<p>Creates an encrypted custom Graph from a pandas DataFrame.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to encrypt node and edge data.</li>\n<li><strong>df</strong> (pd.DataFrame):\nDataFrame containing the edge data.</li>\n<li><strong>from_col</strong> (str):\nColumn name representing the source node.</li>\n<li><strong>to_col</strong> (str):\nColumn name representing the target node.</li>\n<li><strong>weight_col</strong> (str):\nColumn name representing the edge weight.</li>\n<li><strong>use_hashing</strong> (bool, optional, default=True):\nWhether to hash node identifiers.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>Graph</strong>: An encrypted custom Graph with nodes, edges, and outbound counts encrypted.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">ctx</span>,</span><span class=\"param\">\t<span class=\"n\">df</span>,</span><span class=\"param\">\t<span class=\"n\">from_col</span>,</span><span class=\"param\">\t<span class=\"n\">to_col</span>,</span><span class=\"param\">\t<span class=\"n\">weight_col</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">use_hashing</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">directed</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.graphs.venum_graph.decrypt_graph", "modulename": "venumML.graphs.venum_graph", "qualname": "decrypt_graph", "kind": "function", "doc": "<p>Decrypts an encrypted adjacency matrix of an encrypted Graph.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to decrypt values.</li>\n<li><strong>custom_graph</strong> (Graph):\nThe encrypted Graph to decrypt.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>networkx.Graph or networkx.DiGraph</strong>: A decrypted NetworkX graph with nodes and edges restored.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">graph</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.graphs.venum_graph.pagerank", "modulename": "venumML.graphs.venum_graph", "qualname": "pagerank", "kind": "function", "doc": "<p>Computes PageRank scores for an encrypted Graph using the Power Method.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to handle encrypted values.</li>\n<li><strong>encrypted_graph</strong> (Graph):\nThe encrypted custom Graph on which to compute PageRank.</li>\n<li><strong>damping_factor</strong> (float, optional, default=0.85):\nThe probability of following an edge; 1 - damping_factor is the teleport probability.</li>\n<li><strong>iters</strong> (int, optional, default=20):\nThe number of iterations for the Power Method.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>dict</strong>: A dictionary mapping node identifiers to encrypted PageRank scores.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">encrypted_graph</span>, </span><span class=\"param\"><span class=\"n\">damping_factor</span><span class=\"o\">=</span><span class=\"mf\">0.85</span>, </span><span class=\"param\"><span class=\"n\">iters</span><span class=\"o\">=</span><span class=\"mi\">20</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.graphs.venum_graph.decrypt_pagerank", "modulename": "venumML.graphs.venum_graph", "qualname": "decrypt_pagerank", "kind": "function", "doc": "<p>Decrypts encrypted PageRank scores.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to decrypt values.</li>\n<li><strong>encrypted_pagerank</strong> (dict):\nA dictionary mapping node identifiers to encrypted PageRank scores.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>dict</strong>: A dictionary mapping node identifiers to decrypted PageRank scores.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">encrypted_pagerank</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.linear_models", "modulename": "venumML.linear_models", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.linear_models.regression", "modulename": "venumML.linear_models.regression", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.linear_models.regression.linear_regression", "modulename": "venumML.linear_models.regression.linear_regression", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.linear_models.regression.linear_regression.EncryptedLinearRegression", "modulename": "venumML.linear_models.regression.linear_regression", "qualname": "EncryptedLinearRegression", "kind": "class", "doc": "<p>A linear regression model that supports encrypted training and prediction.</p>\n\n<h6 id=\"attributes\">Attributes</h6>\n\n<ul>\n<li><strong>context</strong> (EncryptionContext):\nThe encryption context that provides encryption and decryption methods.</li>\n<li><strong>coef_</strong> (array-like, shape (n_features,)):\nCoefficients of the linear model after fitting (in plaintext).</li>\n<li><strong>intercept_</strong> (float):\nIntercept of the linear model after fitting (in plaintext).</li>\n<li><strong>encrypted_intercept_</strong> (encrypted float):\nEncrypted intercept of the model, used in encrypted prediction.</li>\n<li><strong>encrypted_coef_</strong> (list of encrypted floats):\nEncrypted coefficients of the model, used in encrypted prediction.</li>\n</ul>\n"}, {"fullname": "venumML.linear_models.regression.linear_regression.EncryptedLinearRegression.__init__", "modulename": "venumML.linear_models.regression.linear_regression", "qualname": "EncryptedLinearRegression.__init__", "kind": "function", "doc": "<p>Initialises the EncryptedLinearRegression model with a given encryption context.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to encrypt values.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span></span>)</span>"}, {"fullname": "venumML.linear_models.regression.linear_regression.EncryptedLinearRegression.encrypted_fit", "modulename": "venumML.linear_models.regression.linear_regression", "qualname": "EncryptedLinearRegression.encrypted_fit", "kind": "function", "doc": "<p>Fits the linear regression model on encrypted data using Nesterov's accelerated gradient descent.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to encrypt and decrypt values.</li>\n<li><strong>x</strong> (encrypted array-like, shape (n_samples, n_features)):\nEncrypted input data.</li>\n<li><strong>y</strong> (encrypted array-like, shape (n_samples,)):\nEncrypted target values.</li>\n<li><strong>lr</strong> (float, optional, default=0.3):\nLearning rate for the optimizer.</li>\n<li><strong>gamma</strong> (float, optional, default=0.9):\nMomentum parameter for Nesterov's accelerated gradient descent.</li>\n<li><strong>epochs</strong> (int, optional, default=10):\nNumber of epochs to run for optimization.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.3</span>, </span><span class=\"param\"><span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"mf\">0.9</span>, </span><span class=\"param\"><span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">10</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.linear_models.regression.linear_regression.EncryptedLinearRegression.fit", "modulename": "venumML.linear_models.regression.linear_regression", "qualname": "EncryptedLinearRegression.fit", "kind": "function", "doc": "<p>Fits the linear regression model using ordinary least squares.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>X</strong> (array-like, shape (n_samples, n_features)):\nPlaintext input data.</li>\n<li><strong>y</strong> (array-like, shape (n_samples,)):\nPlaintext target values.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">X</span>, </span><span class=\"param\"><span class=\"n\">y</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.linear_models.regression.linear_regression.EncryptedLinearRegression.encrypt_coefficients", "modulename": "venumML.linear_models.regression.linear_regression", "qualname": "EncryptedLinearRegression.encrypt_coefficients", "kind": "function", "doc": "<p>Encrypts the model's coefficients and intercept after fitting.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to encrypt plaintexts.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">ctx</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.linear_models.regression.linear_regression.EncryptedLinearRegression.predict", "modulename": "venumML.linear_models.regression.linear_regression", "qualname": "EncryptedLinearRegression.predict", "kind": "function", "doc": "<p>Predicts outcomes using encrypted input data and the model's encrypted coefficients.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>encrypted_X</strong> (encrypted array-like, shape (n_samples, n_features)):\nEncrypted input data for making predictions.</li>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to encrypt and decrypt values.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>encrypted_prediction</strong> (encrypted array-like, shape (n_samples,)):\nThe encrypted predictions based on the encrypted model coefficients and intercept.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">encrypted_X</span>, </span><span class=\"param\"><span class=\"n\">ctx</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.linear_models.regression.logistic_regression", "modulename": "venumML.linear_models.regression.logistic_regression", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.linear_models.regression.logistic_regression.EncryptedLogisticRegression", "modulename": "venumML.linear_models.regression.logistic_regression", "qualname": "EncryptedLogisticRegression", "kind": "class", "doc": "<p>Logistic Regression model that supports encrypted computations.</p>\n\n<h6 id=\"attributes\">Attributes</h6>\n\n<ul>\n<li><strong>_context</strong> (object):\nEncryption context to perform encrypted operations.</li>\n<li><strong>_coef_</strong> (numpy.ndarray or None):\nModel coefficients (weights).</li>\n<li><strong>_intercept_</strong> (float):\nIntercept (bias) term.</li>\n<li><strong>_encrypted_coef_</strong> (numpy.ndarray):\nEncrypted model coefficients (weights).</li>\n<li><strong>_encrypted_intercept_</strong> (object):\nEncrypted intercept (bias) term.</li>\n</ul>\n"}, {"fullname": "venumML.linear_models.regression.logistic_regression.EncryptedLogisticRegression.__init__", "modulename": "venumML.linear_models.regression.logistic_regression", "qualname": "EncryptedLogisticRegression.__init__", "kind": "function", "doc": "<p>Initialise the EncryptedLogisticRegression model.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (object):\nEncryption context to handle encrypted operations.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span></span>)</span>"}, {"fullname": "venumML.linear_models.regression.logistic_regression.EncryptedLogisticRegression.fit", "modulename": "venumML.linear_models.regression.logistic_regression", "qualname": "EncryptedLogisticRegression.fit", "kind": "function", "doc": "<p>Fit the model to the provided data using gradient descent.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>X</strong> (numpy.ndarray):\nFeature matrix (n_samples, n_features).</li>\n<li><strong>y</strong> (numpy.ndarray):\nTarget vector (n_samples,).</li>\n<li><strong>num_iterations</strong> (int, optional):\nNumber of iterations for gradient descent (default is 1000).</li>\n<li><strong>learning_rate</strong> (float, optional):\nLearning rate for gradient descent (default is 0.1).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">X</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">num_iterations</span><span class=\"o\">=</span><span class=\"mi\">1000</span>, </span><span class=\"param\"><span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">0.1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.linear_models.regression.logistic_regression.EncryptedLogisticRegression.encrypted_fit", "modulename": "venumML.linear_models.regression.logistic_regression", "qualname": "EncryptedLogisticRegression.encrypted_fit", "kind": "function", "doc": "<p>Fit the model using encrypted data using encrypted data, Nesterov optimization, and a sigmoid approximation.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (object):\nEncryption context for handling encrypted operations.</li>\n<li><strong>x</strong> (numpy.ndarray):\nEncrypted feature matrix (n_samples, n_features).</li>\n<li><strong>y</strong> (numpy.ndarray):\nEncrypted target vector (n_samples,).</li>\n<li><strong>lr</strong> (float, optional):\nLearning rate for optimisation (default is 0.3).</li>\n<li><strong>gamma</strong> (float, optional):\nMomentum term for Nesterov optimisation (default is 0.9).</li>\n<li><strong>epochs</strong> (int, optional):\nNumber of epochs for training (default is 3).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.3</span>, </span><span class=\"param\"><span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"mf\">0.9</span>, </span><span class=\"param\"><span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">3</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.linear_models.regression.logistic_regression.EncryptedLogisticRegression.encrypt_coefficients", "modulename": "venumML.linear_models.regression.logistic_regression", "qualname": "EncryptedLogisticRegression.encrypt_coefficients", "kind": "function", "doc": "<p>Encrypt the model's coefficients and intercept.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (object):\nEncryption context to perform encrypted operations.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">ctx</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.linear_models.regression.logistic_regression.EncryptedLogisticRegression.predict", "modulename": "venumML.linear_models.regression.logistic_regression", "qualname": "EncryptedLogisticRegression.predict", "kind": "function", "doc": "<p>Predict outcomes using encrypted data.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>encrypted_X</strong> (numpy.ndarray):\nEncrypted feature matrix (n_samples, n_features).</li>\n<li><strong>ctx</strong> (object):\nEncryption context for handling encrypted operations.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>object</strong>: Encrypted predictions.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">encrypted_X</span>, </span><span class=\"param\"><span class=\"n\">ctx</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.optimization", "modulename": "venumML.optimization", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.optimization.sgd", "modulename": "venumML.optimization.sgd", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.optimization.sgd.Nesterov", "modulename": "venumML.optimization.sgd", "qualname": "Nesterov", "kind": "class", "doc": "<p>A class implementing Nesterov's Accelerated Gradient Descent (NAGD) for encrypted data.\nThis class leverages homomorphic encryption to securely compute model parameters \nwithout decrypting sensitive data.</p>\n\n<h6 id=\"attributes\">Attributes</h6>\n\n<ul>\n<li><strong>context</strong> (EncryptionContext):\nThe encryption context that provides encryption and decryption methods.</li>\n<li><strong>lr</strong> (float):\nLearning rate for gradient descent.</li>\n<li><strong>gamma</strong> (float):\nMomentum factor for Nesterov's accelerated gradient descent.</li>\n<li><strong>epochs</strong> (int):\nNumber of epochs to run the optimisation.</li>\n</ul>\n"}, {"fullname": "venumML.optimization.sgd.Nesterov.__init__", "modulename": "venumML.optimization.sgd", "qualname": "Nesterov.__init__", "kind": "function", "doc": "<p>Initialises the Nesterov optimiser with an encryption context and optimisation hyperparameters.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nEncryption context used to encrypt values and perform secure computations.</li>\n<li><strong>lr</strong> (float, optional, default=0.3):\nLearning rate for gradient descent.</li>\n<li><strong>gamma</strong> (float, optional, default=0.9):\nMomentum factor for Nesterov's accelerated gradient descent.</li>\n<li><strong>epochs</strong> (int, optional, default=10):\nNumber of epochs to perform the optimisation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.3</span>, </span><span class=\"param\"><span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"mf\">0.9</span>, </span><span class=\"param\"><span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">10</span></span>)</span>"}, {"fullname": "venumML.optimization.sgd.Nesterov.venum_nesterov_agd", "modulename": "venumML.optimization.sgd", "qualname": "Nesterov.venum_nesterov_agd", "kind": "function", "doc": "<p>Applies Nesterov's Accelerated Gradient Descent on encrypted data to optimise weights.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to encrypt values.</li>\n<li><strong>x</strong> (encrypted array-like, shape (n_samples, n_features)):\nEncrypted input data.</li>\n<li><strong>y</strong> (encrypted array-like, shape (n_samples, 1)):\nEncrypted target values.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>encrypted_intercept</strong> (encrypted float):\nThe encrypted intercept term after optimisation.</li>\n<li><strong>encrypted_coef</strong> (encrypted array-like, shape (n_features,)):\nThe encrypted coefficient(s) after optimisation.</li>\n<li><strong>losses</strong> (list of float):\nList of loss values recorded at each epoch.</li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>This method initialises the model's parameters with random values, encrypts them, \nand then iteratively updates them using Nesterov's accelerated gradient descent.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.time_series", "modulename": "venumML.time_series", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.time_series.Phineus", "modulename": "venumML.time_series.Phineus", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.time_series.Phineus.phineus_FFT", "modulename": "venumML.time_series.Phineus.phineus_FFT", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.time_series.Phineus.phineus_FFT.generate_twiddle_factors", "modulename": "venumML.time_series.Phineus.phineus_FFT", "qualname": "generate_twiddle_factors", "kind": "function", "doc": "<p>Generates the twiddle factors for the Fast Fourier Transform (FFT) as tuples of real and imaginary parts.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>n</strong> (int):\nLength of the sequence.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>list of tuple</strong>: Twiddle factors represented as tuples of (real, imaginary) values.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">n</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.time_series.Phineus.phineus_FFT.next_power_of_2", "modulename": "venumML.time_series.Phineus.phineus_FFT", "qualname": "next_power_of_2", "kind": "function", "doc": "<p>Calculate the next power of 2 greater than or equal to n.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>n</strong> (int):\nThe input integer.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>int</strong>: The smallest power of 2 greater than or equal to n.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">n</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.time_series.Phineus.phineus_FFT.pad_ciphervec", "modulename": "venumML.time_series.Phineus.phineus_FFT", "qualname": "pad_ciphervec", "kind": "function", "doc": "<p>Pads the input encrypted vector to a specified length using a padding value.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to encrypt padding values.</li>\n<li><strong>x</strong> (np.ndarray):\nEncrypted input vector.</li>\n<li><strong>pad_length</strong> (int, optional):\nDesired length of the padded vector.</li>\n<li><strong>pad_value</strong> (int, optional, default=0):\nValue to use for padding the vector, which will be encrypted.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>np.ndarray</strong>: Encrypted padded vector.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">pad_length</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">pad_value</span><span class=\"o\">=</span><span class=\"mi\">0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.time_series.Phineus.phineus_FFT.hann_window", "modulename": "venumML.time_series.Phineus.phineus_FFT", "qualname": "hann_window", "kind": "function", "doc": "<p>Applies a Hann window to the input encrypted signal to reduce spectral leakage.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to encrypt window values.</li>\n<li><strong>x</strong> (np.ndarray):\nEncrypted input signal.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>np.ndarray</strong>: The input signal multiplied by the encrypted Hann window.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.time_series.Phineus.phineus_FFT.FFT", "modulename": "venumML.time_series.Phineus.phineus_FFT", "qualname": "FFT", "kind": "function", "doc": "<p>Computes the Fast Fourier Transform (FFT) of an encrypted signal using the 1D Cooley-Tukey algorithm,\na recursive implementation of the 1D Cooley-Tukey FFT without using complex numbers.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to encrypt data and twiddle factors.</li>\n<li><strong>x_unpadded</strong> (np.ndarray):\nEncrypted input signal.</li>\n<li><strong>pad_data</strong> (bool, optional, default=False):\nWhether to pad the input data to the next power of 2 for FFT optimisation.</li>\n<li><strong>window_data</strong> (bool, optional, default=False):\nWhether to apply a Hann window to the input data before the FFT.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>list of tuple</strong>: The FFT of the input signal as a list of tuples (real, imaginary).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">x_unpadded</span>, </span><span class=\"param\"><span class=\"n\">pad_data</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">window_data</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.time_series.Phineus.phineus_FFT.rfftfreq", "modulename": "venumML.time_series.Phineus.phineus_FFT", "qualname": "rfftfreq", "kind": "function", "doc": "<p>Returns the Discrete Fourier Transform sample frequencies for real input signals.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>n</strong> (int):\nWindow length.</li>\n<li><strong>d</strong> (float, optional, default=1.0):\nSample spacing (inverse of the sampling rate).</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>np.ndarray</strong>: Array of frequencies.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">n</span>, </span><span class=\"param\"><span class=\"n\">d</span><span class=\"o\">=</span><span class=\"mf\">1.0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.time_series.Phineus.phineus_rolling_average", "modulename": "venumML.time_series.Phineus.phineus_rolling_average", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.time_series.Phineus.phineus_rolling_average.rolling_average", "modulename": "venumML.time_series.Phineus.phineus_rolling_average", "qualname": "rolling_average", "kind": "function", "doc": "<p>Computes the rolling average of an encrypted array with a specified window size.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used to encrypt the initial rolling average array.</li>\n<li><strong>values</strong> (np.ndarray):\nEncrypted array of input values.</li>\n<li><strong>window</strong> (int):\nThe size of the window over which to compute the rolling average.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>np.ndarray</strong>: Encrypted array containing the rolling averages for each position in the input array.</li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>For the first few elements where the window size is not fully available, the function \ncalculates the average over the available elements up to the current position.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">values</span>, </span><span class=\"param\"><span class=\"n\">window</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.time_series.Phineus.phineus_timeseries", "modulename": "venumML.time_series.Phineus.phineus_timeseries", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.time_series.Phineus.phineus_timeseries.phineus_predict", "modulename": "venumML.time_series.Phineus.phineus_timeseries", "qualname": "phineus_predict", "kind": "function", "doc": "<p>Predicts future values using an encrypted trend and seasonality model.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx</strong> (EncryptionContext):\nThe encryption context used for encrypting data.</li>\n<li><strong>data</strong> (pd.DataFrame):\nInput data containing a 'ds' column for dates and 'y' column for target values.</li>\n<li><strong>forecast_periods</strong> (int, optional, default=30):\nNumber of future periods to forecast.</li>\n<li><strong>frequency</strong> (str, optional, default='D'):\nFrequency of the data (e.g., 'D' for daily).</li>\n<li><strong>window_size</strong> (int, optional, default=30):\nWindow size for rolling average smoothing.</li>\n<li><strong>smoothing</strong> (bool, optional, default=False):\nWhether to apply a rolling average to smooth the data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>fft_values</strong> (list of tuple):\nFFT of the detrended data as tuples of (real, imaginary).</li>\n<li><strong>frequencies</strong> (np.ndarray):\nFrequencies corresponding to the FFT values.</li>\n<li><strong>total_t</strong> (np.ndarray):\nNormalised time points for the forecast period.</li>\n<li><strong>trend_predictions</strong> (np.ndarray):\nForecasted trend values for the total period.</li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>This function models the trend using encrypted linear regression and detrends \nthe data for seasonal analysis with FFT. It can optionally apply a rolling average for smoothing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">ctx</span>,</span><span class=\"param\">\t<span class=\"n\">data</span>,</span><span class=\"param\">\t<span class=\"n\">forecast_periods</span><span class=\"o\">=</span><span class=\"mi\">30</span>,</span><span class=\"param\">\t<span class=\"n\">frequency</span><span class=\"o\">=</span><span class=\"s1\">&#39;D&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">window_size</span><span class=\"o\">=</span><span class=\"mi\">30</span>,</span><span class=\"param\">\t<span class=\"n\">smoothing</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.time_series.Phineus.phineus_timeseries.reconstruct_signal", "modulename": "venumML.time_series.Phineus.phineus_timeseries", "qualname": "reconstruct_signal", "kind": "function", "doc": "<p>Reconstructs a signal from its Fourier components using the largest frequencies.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>amplitudes</strong> (np.ndarray):\nArray of amplitude values from the FFT.</li>\n<li><strong>phases</strong> (np.ndarray):\nArray of phase values corresponding to the FFT frequencies.</li>\n<li><strong>frequencies</strong> (np.ndarray):\nArray of frequencies corresponding to the FFT values.</li>\n<li><strong>t</strong> (np.ndarray):\nTime points for reconstructing the signal.</li>\n<li><strong>number_of_frequencies</strong> (int, optional, default=10):\nNumber of largest frequency components to use for reconstruction.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>np.ndarray</strong>: Reconstructed signal values for each time point in t.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">amplitudes</span>, </span><span class=\"param\"><span class=\"n\">phases</span>, </span><span class=\"param\"><span class=\"n\">frequencies</span>, </span><span class=\"param\"><span class=\"n\">t</span>, </span><span class=\"param\"><span class=\"n\">number_of_frequencies</span><span class=\"o\">=</span><span class=\"mi\">10</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.time_series.Phineus.phineus_timeseries.extend_date_column", "modulename": "venumML.time_series.Phineus.phineus_timeseries", "qualname": "extend_date_column", "kind": "function", "doc": "<p>Extends a date column by a specified number of periods.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>data</strong> (pd.DataFrame):\nDataFrame containing the original date column.</li>\n<li><strong>date_column</strong> (str):\nName of the date column in the DataFrame.</li>\n<li><strong>extension_count</strong> (int):\nNumber of periods to extend the date column by.</li>\n<li><strong>frequency</strong> (str, optional, default='D'):\nFrequency of the dates to be generated (e.g., 'D' for daily).</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>pd.DataFrame</strong>: A new DataFrame with the extended date column.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">data</span>, </span><span class=\"param\"><span class=\"n\">date_column</span>, </span><span class=\"param\"><span class=\"n\">extension_count</span>, </span><span class=\"param\"><span class=\"n\">frequency</span><span class=\"o\">=</span><span class=\"s1\">&#39;D&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.venum_tools", "modulename": "venumML.venum_tools", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.venum_tools.encrypt_array", "modulename": "venumML.venum_tools", "qualname": "encrypt_array", "kind": "function", "doc": "<p>Recursively encrypts every value in an n-dimensional numpy array.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>array</strong> (np.ndarray):\nThe n-dimensional numpy array to encrypt.</li>\n<li><strong>ctx</strong> (EncryptionContext):\nEncryption context providing an encrypt method.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>np.ndarray</strong>: An n-dimensional numpy array with all values encrypted.</li>\n</ul>\n\n<h6 id=\"raises\">Raises</h6>\n\n<ul>\n<li><strong>ValueError</strong>: If the input is not a numpy array.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">array</span>, </span><span class=\"param\"><span class=\"n\">ctx</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.venum_tools.decrypt_array", "modulename": "venumML.venum_tools", "qualname": "decrypt_array", "kind": "function", "doc": "<p>Recursively decrypts every value in an n-dimensional numpy array of encrypted objects.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>array</strong> (np.ndarray):\nThe n-dimensional numpy array containing encrypted objects to decrypt.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>np.ndarray</strong>: An n-dimensional numpy array with all values decrypted.</li>\n</ul>\n\n<h6 id=\"raises\">Raises</h6>\n\n<ul>\n<li><strong>ValueError</strong>: If the input is not a numpy array.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">array</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.venumpy", "modulename": "venumML.venumpy", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.venumpy.small_glwe", "modulename": "venumML.venumpy.small_glwe", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "venumML.venumpy.small_glwe.Ciphertext", "modulename": "venumML.venumpy.small_glwe", "qualname": "Ciphertext", "kind": "class", "doc": "<p>Represents an encrypted ciphertext in the cryptographic scheme.</p>\n\n<p>This class allows for homomorphic operations (addition, subtraction, \nmultiplication) on ciphertexts and supports serialization to/from JSON format.</p>\n\n<h6 id=\"attributes\">Attributes</h6>\n\n<ul>\n<li><strong>_raw_cipher</strong> (object):\nThe underlying raw ciphertext from the bindings.</li>\n<li><strong>_context</strong> (object):\nThe context in which the ciphertext was created (either public or secret).</li>\n</ul>\n"}, {"fullname": "venumML.venumpy.small_glwe.Ciphertext.__init__", "modulename": "venumML.venumpy.small_glwe", "qualname": "Ciphertext.__init__", "kind": "function", "doc": "<p>Initializes a new ciphertext from a raw ciphertext and context.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>raw_cipher</strong> (object):\nThe raw ciphertext object from the bindings.</li>\n<li><strong>context</strong> (object):\nThe context associated with the ciphertext.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">raw_cipher</span>, </span><span class=\"param\"><span class=\"n\">context</span></span>)</span>"}, {"fullname": "venumML.venumpy.small_glwe.Ciphertext.decrypt", "modulename": "venumML.venumpy.small_glwe", "qualname": "Ciphertext.decrypt", "kind": "function", "doc": "<p>Decrypts a ciphertext.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>object</strong>: The decrypted value.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.venumpy.small_glwe.Ciphertext.into_json", "modulename": "venumML.venumpy.small_glwe", "qualname": "Ciphertext.into_json", "kind": "function", "doc": "<p>Serializes the ciphertext into a JSON format.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>str</strong>: A JSON string representation of the ciphertext.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.venumpy.small_glwe.Ciphertext.from_json", "modulename": "venumML.venumpy.small_glwe", "qualname": "Ciphertext.from_json", "kind": "function", "doc": "<p>Deserializes a JSON representation of a ciphertext.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>context</strong> (object):\nThe context to associate with the deserialized Ciphertext.</li>\n<li><strong>cipher_json</strong> (str):\nThe JSON string representing the ciphertext.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>Ciphertext</strong>: A new Ciphertext object.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">context</span>, </span><span class=\"param\"><span class=\"n\">cipher_json</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.venumpy.small_glwe.Ciphertext.into_bytes", "modulename": "venumML.venumpy.small_glwe", "qualname": "Ciphertext.into_bytes", "kind": "function", "doc": "<p>Serializes the ciphertext into bytes.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>bytes</strong>: A byte representation of the ciphertext.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.venumpy.small_glwe.Ciphertext.from_bytes", "modulename": "venumML.venumpy.small_glwe", "qualname": "Ciphertext.from_bytes", "kind": "function", "doc": "<p>Deserializes a byte representation of a ciphertext.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>context</strong> (object):\nThe context to associate with the deserialized Ciphertext.</li>\n<li><strong>cipher_bytes</strong> (bytes):\nThe bytes representing the ciphertext.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>Ciphertext</strong>: A new Ciphertext object.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">context</span>, </span><span class=\"param\"><span class=\"n\">cipher_bytes</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.venumpy.small_glwe.Context", "modulename": "venumML.venumpy.small_glwe", "qualname": "Context", "kind": "class", "doc": "<p>Represents a general cryptographic context.</p>\n\n<p>Provides methods to serialize and manage the precision of computations.</p>\n\n<h6 id=\"attributes\">Attributes</h6>\n\n<ul>\n<li><strong>_raw_ctx</strong> (object):\nThe raw context object from the bindings.</li>\n</ul>\n"}, {"fullname": "venumML.venumpy.small_glwe.Context.into_json", "modulename": "venumML.venumpy.small_glwe", "qualname": "Context.into_json", "kind": "function", "doc": "<p>Serializes the context into a JSON format.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>str</strong>: A JSON string representation of the context.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.venumpy.small_glwe.Context.into_bytes", "modulename": "venumML.venumpy.small_glwe", "qualname": "Context.into_bytes", "kind": "function", "doc": "<p>Serializes the context into a byte format.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>bytes</strong>: A byte representation of the context.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.venumpy.small_glwe.Context.precision", "modulename": "venumML.venumpy.small_glwe", "qualname": "Context.precision", "kind": "variable", "doc": "<p>Gets the floating point precision of the context.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>int</strong>: The current precision.</li>\n</ul>\n"}, {"fullname": "venumML.venumpy.small_glwe.SecretContext", "modulename": "venumML.venumpy.small_glwe", "qualname": "SecretContext", "kind": "class", "doc": "<p>Represents a secret cryptographic context, used for key generation and decryption.</p>\n\n<h6 id=\"attributes\">Attributes</h6>\n\n<ul>\n<li><strong>_raw_ctx</strong> (object):\nThe underlying raw secret context from the bindings.</li>\n</ul>\n", "bases": "Context"}, {"fullname": "venumML.venumpy.small_glwe.SecretContext.__init__", "modulename": "venumML.venumpy.small_glwe", "qualname": "SecretContext.__init__", "kind": "function", "doc": "<p>Initializes a new secret context.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>bits_of_security</strong> (int, optional):\nThe bits of security for the cryptographic context (default is 128).</li>\n<li><strong>generate</strong> (bool, optional):\nWhether to generate a new secret context (default is True).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">bits_of_security</span><span class=\"o\">=</span><span class=\"mi\">128</span>, </span><span class=\"param\"><span class=\"n\">generate</span><span class=\"o\">=</span><span class=\"kc\">True</span></span>)</span>"}, {"fullname": "venumML.venumpy.small_glwe.SecretContext.encrypt", "modulename": "venumML.venumpy.small_glwe", "qualname": "SecretContext.encrypt", "kind": "function", "doc": "<p>Encrypts a plaintext value using this secret context.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>plaintext</strong> (object):\nThe value to encrypt.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>Ciphertext</strong>: The encrypted value as a Ciphertext object.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">plaintext</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.venumpy.small_glwe.SecretContext.decrypt", "modulename": "venumML.venumpy.small_glwe", "qualname": "SecretContext.decrypt", "kind": "function", "doc": "<p>Decrypts a ciphertext using this secret context.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ciphertext</strong> (Ciphertext):\nThe Ciphertext object to decrypt.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>object</strong>: The decrypted value.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">ciphertext</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.venumpy.small_glwe.SecretContext.into_public", "modulename": "venumML.venumpy.small_glwe", "qualname": "SecretContext.into_public", "kind": "function", "doc": "<p>Converts this secret context into a public context.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>PublicContext</strong>: A new PublicContext object.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.venumpy.small_glwe.SecretContext.from_json", "modulename": "venumML.venumpy.small_glwe", "qualname": "SecretContext.from_json", "kind": "function", "doc": "<p>Deserializes a secret context from a JSON string.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx_json</strong> (str):\nThe JSON string representing the secret context.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>SecretContext</strong>: A new SecretContext object.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">ctx_json</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.venumpy.small_glwe.SecretContext.from_bytes", "modulename": "venumML.venumpy.small_glwe", "qualname": "SecretContext.from_bytes", "kind": "function", "doc": "<p>Deserializes a secret context from byte representation.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx_bytes</strong> (bytes):\nThe bytes representing the secret context.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>SecretContext</strong>: A new SecretContext object.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">ctx_bytes</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.venumpy.small_glwe.PublicContext", "modulename": "venumML.venumpy.small_glwe", "qualname": "PublicContext", "kind": "class", "doc": "<p>Represents a public cryptographic context, used for encryption.</p>\n\n<p>A <code>PublicContext</code> is generated from a <code>SecretContext</code> and allows for\nencryption operations, but not decryption.</p>\n\n<h6 id=\"attributes\">Attributes</h6>\n\n<ul>\n<li><strong>_raw_ctx</strong> (object):\nThe raw public context object from the bindings.</li>\n</ul>\n", "bases": "Context"}, {"fullname": "venumML.venumpy.small_glwe.PublicContext.__init__", "modulename": "venumML.venumpy.small_glwe", "qualname": "PublicContext.__init__", "kind": "function", "doc": "<p>Initializes a new public context.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>generate</strong> (bool, optional):\nWhether to generate a new public context (default is False).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">generate</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "venumML.venumpy.small_glwe.PublicContext.encrypt", "modulename": "venumML.venumpy.small_glwe", "qualname": "PublicContext.encrypt", "kind": "function", "doc": "<p>Encrypts the provided plaintext values.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>plaintext</strong> (object):\nThe value to encrypt.</li>\n</ul>\n\n<h6 id=\"raises\">Raises</h6>\n\n<ul>\n<li><strong>Exception</strong>: Raised because public encryption is currently not supported.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>None</strong></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">plaintext</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.venumpy.small_glwe.PublicContext.decrypt", "modulename": "venumML.venumpy.small_glwe", "qualname": "PublicContext.decrypt", "kind": "function", "doc": "<p>Decrypts the provided ciphertext.</p>\n\n<h6 id=\"raises\">Raises</h6>\n\n<ul>\n<li><strong>Exception</strong>: Raised because public decryption is currently not supported.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>None</strong></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">ciphertext</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.venumpy.small_glwe.PublicContext.from_json", "modulename": "venumML.venumpy.small_glwe", "qualname": "PublicContext.from_json", "kind": "function", "doc": "<p>Deserializes a public context from a JSON string.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx_json</strong> (str):\nThe JSON string representing the public context.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>PublicContext</strong>: A new PublicContext object.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">ctx_json</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "venumML.venumpy.small_glwe.PublicContext.from_bytes", "modulename": "venumML.venumpy.small_glwe", "qualname": "PublicContext.from_bytes", "kind": "function", "doc": "<p>Deserializes a public context from byte representation.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>ctx_bytes</strong> (bytes):\nThe bytes representing the public context.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>PublicContext</strong>: A new PublicContext object.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">ctx_bytes</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();